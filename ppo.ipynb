{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OpenAIGym\n",
    "using Flux\n",
    "import Statistics: mean\n",
    "import StatsBase:sample \n",
    "using Flux: onehotbatch\n",
    "import Distributions: Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(6, 64, tanh), Dense(64, 64, tanh), Dense(64, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GymEnv(:Acrobot, :v1)\n",
    "\n",
    "# env parameters\n",
    "n_actions = length(env.actions)\n",
    "n_states = length(env.state)\n",
    "\n",
    "#models\n",
    "mlpvalue = Chain(Dense(n_states, 64, tanh), Dense(64, 64, tanh), Dense(64, 1))\n",
    "mlppolicy = Chain(Dense(n_states, 64, tanh), Dense(64, 64, tanh), Dense(64, n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_categorical_policy (generic function with 2 methods)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp_categorical_policy(x, hidden_sizes, activation, output_activation, act_dim, a = nothing)    \n",
    "    logits = mlppolicy(x)\n",
    "    logp_all = logsoftmax(logits)\n",
    "    \n",
    "    if typeof(logits.data) != Array{Float64,2}\n",
    "        dist = Multinomial(1, softmax(logits.data))\n",
    "        sample = rand(dist, 1)\n",
    "\n",
    "        pi = Flux.argmax(dropdims(sample, dims=2))\n",
    "        if a == nothing\n",
    "            logp = nothing\n",
    "        else\n",
    "            logp = sum(Flux.onehot(a, 1:act_dim) .* logp_all, dims=1)\n",
    "        end\n",
    "        logp_pi = sum(Flux.onehot(pi, 1:act_dim) .* logp_all, dims = 1)\n",
    "        return pi, logp, logp_pi\n",
    "    else\n",
    "        pi = []\n",
    "        for i in 1:size(logits.data)[2]\n",
    "            k = logits[:, i]\n",
    "            dist = Multinomial(1, softmax(k.data))\n",
    "            sample = rand(dist, 1)\n",
    "            push!(pi, Flux.argmax(dropdims(sample, dims=2)))\n",
    "        end\n",
    "        if a == nothing\n",
    "            logp = nothing\n",
    "        else\n",
    "            logp = sum(Flux.onehotbatch(a, 1:act_dim) .* logp_all, dims = 1)\n",
    "        end\n",
    "        logp_pi = sum(Flux.onehotbatch(pi, 1:act_dim) .* logp_all, dims = 1)\n",
    "        return pi, logp', logp_pi'\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mlp_actor_critic (generic function with 6 methods)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mlp_actor_critic(x, act_dim, a = nothing, hidden_sizes=(64,64), \n",
    "                          activation=tanh, output_activation=nothing, \n",
    "                          policy=nothing)\n",
    "\n",
    "    # default policy builder depends on action space\n",
    "    if policy == nothing \n",
    "        policy = mlp_categorical_policy\n",
    "    end\n",
    "    \n",
    "    pi, logp, logp_pi = policy(x, hidden_sizes, activation, output_activation, act_dim, a)\n",
    "    \n",
    "    v = mlpvalue(x)\n",
    "    return pi, logp, logp_pi, v\n",
    "    end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "update (generic function with 1 method)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function pi_loss(x_ph, act_dim, a_ph, steps_per_epoch, adv_ph, logp_old_ph)\n",
    "    pi, logp, logp_pi, v = mlp_actor_critic(x_ph, act_dim, a_ph)\n",
    "    ratio = exp.(logp - logp_old_ph)\n",
    "    min_adv = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    for i in 1:length(adv_ph)\n",
    "        if adv_ph[i, 1] > 0\n",
    "            min_adv[i, 1] = (1+0.2)*adv_ph[i, 1]\n",
    "        else\n",
    "            min_adv[i, 1] = (1-0.2)*adv_ph[i, 1]\n",
    "        end\n",
    "    end\n",
    "    return -mean(min.(ratio .* adv_ph, min_adv))\n",
    "end\n",
    "\n",
    "function v_loss(x_ph, act_dim, a_ph, ret_ph)\n",
    "    pi, logp, logp_pi, v = mlp_actor_critic(x_ph, act_dim, a_ph)\n",
    "    return mean((dropdims(ret_ph, dims = 2) - dropdims(v, dims = 1)).^2)\n",
    "end\n",
    "\n",
    "function update(x_ph, a_ph, adv_ph, ret_ph, logp_old_ph, \n",
    "                act_dim, train_pi_iters, train_v_iters, target_kl, steps_per_epoch, pi_lr, vf_lr) \n",
    "    train_pi = Flux.ADAM(Flux.params(mlppolicy), pi_lr)\n",
    "    train_v = Flux.ADAM(Flux.params(mlpvalue), vf_lr)\n",
    "\n",
    "\n",
    "    # Training\n",
    "    for i in 1:train_pi_iters\n",
    "        Flux.train!(pi_loss, [(x_ph, act_dim, a_ph, steps_per_epoch, adv_ph, logp_old_ph)], train_pi)\n",
    "    end\n",
    "        \n",
    "    for _ in 1:train_v_iters\n",
    "        Flux.train!(v_loss, [(x_ph, act_dim, a_ph, ret_ph)], train_v)\n",
    "\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "discount_cumsum (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function discount_cumsum(arr, discount)\n",
    "    \"\"\"\n",
    "    magic from rllab for computing discounted cumulative sums of vectors.\n",
    "    input: \n",
    "        vector x, \n",
    "        [x0, \n",
    "         x1, \n",
    "         x2]\n",
    "    output:\n",
    "        [x0 + discount * x1 + discount^2 * x2,  1\n",
    "         x1 + discount * x2,                    2\n",
    "         x2]                                    3\n",
    "    \"\"\"\n",
    "    rtg = Array{Float32}(undef, length(arr))\n",
    "    for i in length(arr):-1:1\n",
    "        if i == length(arr)\n",
    "            rtg[i] = arr[i]\n",
    "        else\n",
    "            rtg[i] = arr[i] + discount * rtg[i+1]\n",
    "        end\n",
    "    end\n",
    "    return rtg\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ppo (generic function with 15 methods)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_performance = []\n",
    "function ppo(env, actor_critic=mlp_actor_critic, ac_kwargs=Dict(), seed=0, \n",
    "        steps_per_epoch=4000, epochs=25, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01)\n",
    "    \n",
    "    Random.seed!(1234567)\n",
    "    \n",
    "    act_dim = length(env.actions)\n",
    "    obs_dim = length(env.state)\n",
    "    \n",
    "    #initializing buffer\n",
    "    obs_buf = Matrix{Float32}(undef, obs_dim, steps_per_epoch)\n",
    "    act_buf = zeros(Float32, steps_per_epoch)\n",
    "    adv_buf = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    rew_buf = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    ret_buf = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    val_buf = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    logp_buf = Matrix{Float32}(undef, steps_per_epoch, 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ptr, path_start_idx, max_size = 1, 1, steps_per_epoch\n",
    "    \n",
    "    adv_ph, ret_ph, logp_old_ph = nothing, nothing, nothing\n",
    "\n",
    "    o, r, d, ep_ret, ep_len = reset!(env), 0, false, 0, 0\n",
    "    for epoch ∈ 1:epochs\n",
    "        epoch_p = []\n",
    "        for t ∈ 1: steps_per_epoch\n",
    "            \n",
    "            x_ph = reshape(o, :, 1)\n",
    "            temp = []\n",
    "            for i in 1:length(x_ph)\n",
    "                push!(temp, x_ph[i])\n",
    "            end\n",
    "            x_ph = temp\n",
    "            a, _, logp_t, v_t = mlp_actor_critic(x_ph, act_dim)\n",
    "            v_t = v_t.data\n",
    "            logp_t = logp_t.data\n",
    "\n",
    "            # store in buffer\n",
    "            obs_buf[:, ptr] = convert(Array{Float32},o)\n",
    "            \n",
    "            act_buf[ptr] = a \n",
    "            rew_buf[ptr] = r\n",
    "            val_buf[ptr] = v_t[1]\n",
    "            logp_buf[ptr] = logp_t[1]\n",
    "            ptr += 1\n",
    "            \n",
    "            r, o = step!(env, a-1)\n",
    "            \n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "            \n",
    "            terminal = env.done | (ep_len == max_ep_len)\n",
    "            if terminal || (t==steps_per_epoch)\n",
    "                \n",
    "                if ~terminal\n",
    "                    println(\"Warning: trajectory cut off by epoch at $ep_len steps.\")\n",
    "                end\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if env.done\n",
    "                    last_val = r\n",
    "                else\n",
    "                    temp = []\n",
    "                    temp1 = reshape(o, :, 1)\n",
    "                    for i in 1:length(temp1)\n",
    "                        push!(temp, temp1[i])\n",
    "                    end\n",
    "                    _, _, _, last_val = mlp_actor_critic(temp, act_dim)\n",
    "                    last_val = last_val.data\n",
    "                end\n",
    "                rews = push!(rew_buf[path_start_idx: ptr-1], last_val[1])\n",
    "                vals = push!(val_buf[path_start_idx: ptr-1], last_val[1])\n",
    "                \n",
    "        \n",
    "                # the next two lines implement GAE-Lambda advantage calculation\n",
    "                deltas = rews[1:end-1] + gamma * vals[2:end] - vals[1:end-1]\n",
    "                adv_buf[path_start_idx: ptr-1] = discount_cumsum(deltas, gamma * lam)\n",
    "        \n",
    "                # the next line computes rewards-to-go, to be targets for the value function\n",
    "                ret_buf[path_start_idx: ptr-1] = discount_cumsum(rews, gamma)[1:end-1]\n",
    "                \n",
    "                path_start_idx = ptr\n",
    "                \n",
    "#                 if terminal\n",
    "#                     println(\"Reward $ep_ret , Length $ep_len\")\n",
    "#                 end\n",
    "                push!(epoch_p, ep_ret)\n",
    "                o, r, ep_ret, ep_len = reset!(env), 0, false, 0, 0\n",
    "            end\n",
    "            \n",
    "        end\n",
    "        ptr, path_start_idx = 1, 1\n",
    "        println(epoch, \" \", mean(epoch_p[1:end-1]), \" \", length(epoch_p))\n",
    "        push!(epoch_performance, mean(epoch_p[1:end-1]))\n",
    "        update(obs_buf, act_buf, adv_buf, ret_buf, logp_buf, act_dim, \n",
    "               train_pi_iters, train_v_iters, target_kl, steps_per_epoch,\n",
    "               pi_lr, vf_lr)   \n",
    "    end \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = mlp_categorical_policy(::Array{Any,1}, ::Tuple{Int64,Int64}, ::Function, ::Nothing, ::Int64, ::Nothing) at In[27]:9\n",
      "└ @ Main ./In[27]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 68 steps.\n",
      "1 -491.125 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = mlp_categorical_policy(::Array{Float32,2}, ::Tuple{Int64,Int64}, ::Function, ::Nothing, ::Int64, ::Array{Float32,1}) at In[27]:23\n",
      "└ @ Main ./In[27]:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 266 steps.\n",
      "2 -466.5 9\n",
      "Warning: trajectory cut off by epoch at 50 steps.\n",
      "3 -493.625 9\n",
      "Warning: trajectory cut off by epoch at 113 steps.\n",
      "4 -485.75 9\n",
      "Warning: trajectory cut off by epoch at 181 steps.\n",
      "5 -477.125 9\n",
      "Warning: trajectory cut off by epoch at 106 steps.\n",
      "6 -486.625 9\n",
      "Warning: trajectory cut off by epoch at 377 steps.\n",
      "7 -361.5 11\n",
      "Warning: trajectory cut off by epoch at 499 steps.\n",
      "8 -437.125 9\n",
      "Warning: trajectory cut off by epoch at 149 steps.\n",
      "9 -481.0 9\n",
      "Warning: trajectory cut off by epoch at 375 steps.\n",
      "10 -402.0 10\n",
      "Warning: trajectory cut off by epoch at 131 steps.\n",
      "11 -483.375 9\n",
      "Warning: trajectory cut off by epoch at 260 steps.\n",
      "12 -467.375 9\n",
      "Warning: trajectory cut off by epoch at 22 steps.\n",
      "13 -497.125 9\n",
      "Warning: trajectory cut off by epoch at 112 steps.\n",
      "14 -485.875 9\n",
      "Warning: trajectory cut off by epoch at 295 steps.\n",
      "15 -411.0 10\n",
      "Warning: trajectory cut off by epoch at 52 steps.\n",
      "16 -358.09090909090907 12\n",
      "Warning: trajectory cut off by epoch at 100 steps.\n",
      "17 -299.0 14\n",
      "Warning: trajectory cut off by epoch at 124 steps.\n",
      "18 -214.33333333333334 19\n",
      "Warning: trajectory cut off by epoch at 10 steps."
     ]
    }
   ],
   "source": [
    "env = GymEnv(:Acrobot, :v1)\n",
    "ppo(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Base.Random is deprecated, run `using Random` instead\n",
      "  likely near /Users/parthshah/.julia/packages/IJulia/DL02A/src/kernel.jl:41\n",
      "WARNING: Base.Random is deprecated, run `using Random` instead\n",
      "  likely near /Users/parthshah/.julia/packages/IJulia/DL02A/src/kernel.jl:41\n",
      "WARNING: Base.Random is deprecated, run `using Random` instead\n",
      "  likely near /Users/parthshah/.julia/packages/IJulia/DL02A/src/kernel.jl:41\n",
      "WARNING: Base.Random is deprecated, run `using Random` instead\n",
      "  likely near /Users/parthshah/.julia/packages/IJulia/DL02A/src/kernel.jl:41\n",
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = mlp_categorical_policy(::Array{Any,1}, ::Tuple{Int64,Int64}, ::Function, ::Nothing, ::Int64, ::Nothing) at In[3]:9\n",
      "└ @ Main ./In[3]:9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 49 steps.\n",
      "1 30.16030534351145 132Any[14.0, 64.0, 38.0, 41.0, 37.0, 22.0, 15.0, 26.0, 37.0, 48.0, 23.0, 31.0, 15.0, 15.0, 96.0, 13.0, 22.0, 66.0, 18.0, 12.0, 42.0, 17.0, 23.0, 22.0, 17.0, 19.0, 13.0, 58.0, 37.0, 23.0, 15.0, 34.0, 11.0, 42.0, 16.0, 30.0, 35.0, 57.0, 14.0, 17.0, 21.0, 37.0, 25.0, 22.0, 27.0, 30.0, 26.0, 67.0, 25.0, 18.0, 37.0, 20.0, 27.0, 28.0, 46.0, 19.0, 15.0, 37.0, 16.0, 23.0, 21.0, 36.0, 30.0, 22.0, 86.0, 22.0, 27.0, 17.0, 15.0, 54.0, 38.0, 17.0, 94.0, 23.0, 32.0, 37.0, 15.0, 47.0, 39.0, 16.0, 19.0, 9.0, 21.0, 63.0, 73.0, 22.0, 37.0, 21.0, 37.0, 37.0, 16.0, 45.0, 22.0, 51.0, 24.0, 22.0, 37.0, 64.0, 36.0, 52.0, 15.0, 22.0, 14.0, 13.0, 67.0, 25.0, 19.0, 20.0, 26.0, 29.0, 19.0, 36.0, 18.0, 34.0, 37.0, 16.0, 11.0, 24.0, 13.0, 43.0, 16.0, 43.0, 30.0, 39.0, 38.0, 21.0, 31.0, 16.0, 14.0, 22.0, 23.0, 49.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: `argmax(...) is deprecated, use `onecold(...)` instead.\n",
      "│   caller = mlp_categorical_policy(::Array{Float32,2}, ::Tuple{Int64,Int64}, ::Function, ::Nothing, ::Int64, ::Array{Float32,1}) at In[3]:23\n",
      "└ @ Main ./In[3]:23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: trajectory cut off by epoch at 132 steps.\n",
      "2 50.23376623376623 78Any[35.0, 35.0, 36.0, 33.0, 79.0, 19.0, 21.0, 81.0, 38.0, 29.0, 32.0, 29.0, 88.0, 41.0, 129.0, 51.0, 58.0, 37.0, 100.0, 103.0, 23.0, 108.0, 44.0, 36.0, 24.0, 99.0, 84.0, 13.0, 58.0, 29.0, 52.0, 12.0, 46.0, 27.0, 60.0, 21.0, 72.0, 20.0, 113.0, 18.0, 108.0, 110.0, 58.0, 51.0, 64.0, 55.0, 60.0, 60.0, 33.0, 26.0, 56.0, 33.0, 121.0, 64.0, 32.0, 83.0, 36.0, 85.0, 29.0, 36.0, 33.0, 60.0, 49.0, 24.0, 23.0, 62.0, 14.0, 16.0, 50.0, 21.0, 56.0, 33.0, 60.0, 15.0, 72.0, 26.0, 21.0, 132.0]\n",
      "Warning: trajectory cut off by epoch at 21 steps.\n",
      "3 147.37037037037038 28Any[170.0, 134.0, 112.0, 88.0, 164.0, 144.0, 200.0, 198.0, 128.0, 147.0, 40.0, 111.0, 200.0, 170.0, 34.0, 142.0, 103.0, 200.0, 141.0, 41.0, 200.0, 200.0, 200.0, 157.0, 200.0, 162.0, 193.0, 21.0]\n",
      "Warning: trajectory cut off by epoch at 101 steps.\n",
      "4 185.66666666666666 22Any[200.0, 161.0, 200.0, 187.0, 200.0, 200.0, 187.0, 139.0, 140.0, 200.0, 200.0, 200.0, 178.0, 198.0, 200.0, 168.0, 200.0, 200.0, 184.0, 194.0, 163.0, 101.0]\n",
      "Warning: trajectory cut off by epoch at 53 steps.\n",
      "5 187.95238095238096 22Any[200.0, 200.0, 178.0, 200.0, 200.0, 180.0, 171.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 133.0, 200.0, 200.0, 171.0, 188.0, 200.0, 126.0, 53.0]\n",
      "Warning: trajectory cut off by epoch at 123 steps.\n",
      "6 193.85 21Any[200.0, 200.0, 191.0, 189.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 184.0, 200.0, 200.0, 156.0, 157.0, 200.0, 200.0, 200.0, 200.0, 200.0, 123.0]\n",
      "Warning: trajectory cut off by epoch at 184 steps.\n",
      "7 190.8 21Any[186.0, 146.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 135.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 157.0, 200.0, 200.0, 200.0, 184.0]\n",
      "Warning: trajectory cut off by epoch at 30 steps.\n",
      "8 198.5 21Any[200.0, 200.0, 186.0, 200.0, 200.0, 200.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 192.0, 200.0, 200.0, 200.0, 200.0, 30.0]\n",
      "Warning: trajectory cut off by epoch at 57 steps.\n",
      "9 197.15 21Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 164.0, 200.0, 200.0, 200.0, 179.0, 200.0, 200.0, 57.0]\n",
      "Warning: trajectory cut off by epoch at 4 steps.\n",
      "10 199.8 21Any[200.0, 200.0, 200.0, 200.0, 196.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 4.0]\n",
      "11 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "Warning: trajectory cut off by epoch at 10 steps.\n",
      "12 199.5 21Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 10.0]\n",
      "Warning: trajectory cut off by epoch at 10 steps.\n",
      "13 199.5 21Any[200.0, 200.0, 200.0, 200.0, 200.0, 190.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 10.0]\n",
      "Warning: trajectory cut off by epoch at 13 steps.\n",
      "14 199.35 21Any[200.0, 200.0, 200.0, 200.0, 187.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 13.0]\n",
      "15 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "16 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "17 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "18 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "19 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n",
      "20 200.0 20Any[200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0, 200.0]\n"
     ]
    }
   ],
   "source": [
    "env = GymEnv(:CartPole, :v0)\n",
    "ppo(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.7.0",
   "language": "julia",
   "name": "julia-0.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
